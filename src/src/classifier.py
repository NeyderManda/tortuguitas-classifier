import torch
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig

# Configuraci√≥n del Modelo (BART Large MNLI es excelente para clasificaci√≥n Zero-Shot)
MODEL_NAME = "facebook/bart-large-mnli"

def load_model():
    """
    Carga el modelo en la GPU usando cuantizaci√≥n de 4-bits (NF4).
    Esta funci√≥n est√° dise√±ada para correr en la PC de la universidad (Linux/WSL + GPU).
    """
    print(f"‚è≥ Cargando modelo {MODEL_NAME} en la GPU...")
    
    # 1. Configuraci√≥n de Cuantizaci√≥n (Para ahorrar VRAM)
    # Esto reduce el modelo para que quepa holgadamente en los 12GB
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    # 2. Cargar Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    # 3. Cargar Modelo con configuraci√≥n de GPU
    try:
        model = AutoModelForSequenceClassification.from_pretrained(
            MODEL_NAME,
            quantization_config=bnb_config, # ¬°Aqu√≠ ocurre la magia de QLoRA!
            device_map="auto" # Reparte el modelo autom√°ticamente en la GPU
        )
    except Exception as e:
        print(f"‚ö†Ô∏è Error cargando BitsAndBytes (Normal en PC de casa): {e}")
        print("Cargando en modo CPU (Lento, solo para pruebas)...")
        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

    # 4. Crear Pipeline de Clasificaci√≥n
    classifier = pipeline(
        "zero-shot-classification",
        model=model,
        tokenizer=tokenizer
    )
    
    print("‚úÖ Modelo cargado exitosamente.")
    return classifier

# Instancia global (para no recargar el modelo en cada clic)
# Ma√±ana en la universidad, al importar este script, se cargar√° el modelo.
AI_CLASSIFIER = None

def classify_document(text, candidate_labels):
    """
    Recibe texto y posibles categor√≠as. Devuelve los puntajes.
    """
    global AI_CLASSIFIER
    
    if AI_CLASSIFIER is None:
        # Carga perezosa (Lazy loading)
        AI_CLASSIFIER = load_model()

    # Truncar texto si es muy largo (BART tiene l√≠mite de 1024 tokens)
    # Tomamos los primeros 2000 caracteres como aproximaci√≥n r√°pida
    text_to_process = text[:2000]

    print("üß† Procesando con Transformer...")
    result = AI_CLASSIFIER(text_to_process, candidate_labels)
    
    # Formatear resultado para Gradio (Diccionario {Etiqueta: Puntaje})
    output_scores = {}
    for label, score in zip(result['labels'], result['scores']):
        output_scores[label] = score
        
    return output_scores